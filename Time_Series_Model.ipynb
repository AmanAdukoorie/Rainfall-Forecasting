{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6492f2ca"
      },
      "source": [
        "After running the above cell and resolving the compatibility issue, you can re-run the import cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "import time\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe3yk9hotdiy",
        "outputId": "885bb8e8-21ff-4fdf-c446-4b9a499b3829"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All imports successful!\n",
            "NumPy version: 1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = pd.read_csv('/Standardised Data.csv')"
      ],
      "metadata": {
        "id": "DyO8_AOg1DxD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Fill remaining missing values using forward fill, then backward fill\n",
        "df_filtered = df_filtered.ffill().bfill()\n",
        "\n",
        "# Convert 'YEAR' to datetime for creating a time series index\n",
        "df_filtered['Date'] = pd.to_datetime(df_filtered['YEAR'], format='%Y')\n",
        "\n",
        "# Set 'Date' as the index\n",
        "df_filtered = df_filtered.set_index('Date')\n",
        "\n",
        "# Identify month columns\n",
        "expected_month_columns = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
        "month_columns = [col for col in expected_month_columns if col in df_filtered.columns]\n",
        "\n",
        "# Melt the DataFrame to unpivot the month columns into rows\n",
        "df_reset = df_filtered.reset_index()\n",
        "df_melted = df_reset.melt(id_vars=['Date', 'YEAR', 'SUBDIVISION', 'ANNUAL'],\n",
        "                          value_vars=month_columns,\n",
        "                          var_name='Month',\n",
        "                          value_name='Rainfall')\n",
        "\n",
        "# Map month names to numerical month values\n",
        "month_to_num = {name: i+1 for i, name in enumerate(expected_month_columns)}\n",
        "df_melted['Month_Num'] = df_melted['Month'].map(month_to_num)\n",
        "\n",
        "# Create a full monthly datetime index\n",
        "df_melted['Full_Date'] = pd.to_datetime(df_melted['YEAR'].astype(str) + '-' + df_melted['Month_Num'].astype(str) + '-01')\n",
        "\n",
        "# Set 'Full_Date' as index and sort\n",
        "df_melted = df_melted.set_index('Full_Date').sort_index()\n",
        "\n",
        "# Fill NaNs in 'Rainfall' column of df_melted before aggregation or model training\n",
        "df_melted['Rainfall'] = df_melted['Rainfall'].ffill().bfill()\n",
        "\n",
        "# Get unique subdivisions\n",
        "subdivisions = df_melted['SUBDIVISION'].unique()\n",
        "\n",
        "# Define the full date range for reindexing\n",
        "full_date_range = pd.date_range(start='1901-01-01', end='2015-12-01', freq='MS')\n",
        "\n",
        "def time_series_cv_split(data, n_splits=3):\n",
        "    \"\"\"\n",
        "    Create time series cross-validation splits using forward chaining.\n",
        "    Each fold uses progressively more data for training.\n",
        "    \"\"\"\n",
        "    total_length = len(data)\n",
        "    min_train_size = total_length // (n_splits + 1)  # Minimum training size\n",
        "\n",
        "    splits = []\n",
        "    for i in range(n_splits):\n",
        "        # Training set grows with each fold\n",
        "        train_end_idx = min_train_size + (i + 1) * (total_length - min_train_size) // n_splits\n",
        "        val_start_idx = train_end_idx\n",
        "        val_end_idx = min(train_end_idx + min_train_size // 2, total_length)\n",
        "\n",
        "        if val_end_idx <= val_start_idx:\n",
        "            break\n",
        "\n",
        "        train_data = data.iloc[:train_end_idx]\n",
        "        val_data = data.iloc[val_start_idx:val_end_idx]\n",
        "\n",
        "        splits.append((train_data, val_data))\n",
        "\n",
        "    return splits\n",
        "\n",
        "def evaluate_hk_sarima_cv(ts_data, n_splits=3):\n",
        "    \"\"\"\n",
        "    Evaluate SARIMA model using Hyndman-Khandakar algorithm with time series cross-validation.\n",
        "    \"\"\"\n",
        "    cv_scores = {'mae': [], 'rmse': []}\n",
        "    selected_models = []\n",
        "\n",
        "    try:\n",
        "        # Create time series CV splits\n",
        "        cv_splits = time_series_cv_split(ts_data, n_splits)\n",
        "\n",
        "        for fold_idx, (train_data, val_data) in enumerate(cv_splits):\n",
        "            if len(train_data) < 24 or len(val_data) < 1:  # Need minimum data for seasonal model\n",
        "                continue\n",
        "\n",
        "            # Check for NaNs\n",
        "            if train_data.isnull().sum() > 0 or val_data.isnull().sum() > 0:\n",
        "                continue\n",
        "\n",
        "            # Apply Hyndman-Khandakar algorithm using pmdarima\n",
        "            model = auto_arima(train_data,\n",
        "                              seasonal=True,\n",
        "                              m=12,  # Monthly seasonality\n",
        "                              max_p=2, max_d=2, max_q=2,\n",
        "                              max_P=2, max_D=1, max_Q=2,\n",
        "                              stepwise=True,\n",
        "                              suppress_warnings=True,\n",
        "                              error_action='ignore',\n",
        "                              information_criterion='aic',\n",
        "                              n_fits=50,  # Limit search space\n",
        "                              start_p=0, start_q=0,\n",
        "                              start_P=0, start_Q=0)\n",
        "\n",
        "            # Store selected model parameters for analysis\n",
        "            selected_models.append({\n",
        "                'fold': fold_idx,\n",
        "                'order': model.order,\n",
        "                'seasonal_order': model.seasonal_order,\n",
        "                'aic': model.aic()\n",
        "            })\n",
        "\n",
        "            # Generate forecasts\n",
        "            forecast = model.predict(n_periods=len(val_data))\n",
        "\n",
        "            # Calculate metrics\n",
        "            mae = mean_absolute_error(val_data, forecast)\n",
        "            rmse = np.sqrt(mean_squared_error(val_data, forecast))\n",
        "\n",
        "            cv_scores['mae'].append(mae)\n",
        "            cv_scores['rmse'].append(rmse)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in CV evaluation: {e}\")\n",
        "        return None, None, []\n",
        "\n",
        "    if len(cv_scores['mae']) > 0:\n",
        "        avg_mae = np.mean(cv_scores['mae'])\n",
        "        avg_rmse = np.mean(cv_scores['rmse'])\n",
        "        return avg_mae, avg_rmse, selected_models\n",
        "    else:\n",
        "        return None, None, []\n",
        "\n",
        "def fit_final_hk_sarima(train_data):\n",
        "    \"\"\"\n",
        "    Fit final SARIMA model using Hyndman-Khandakar algorithm on full training data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = auto_arima(train_data,\n",
        "                          seasonal=True,\n",
        "                          m=12,  # Monthly seasonality\n",
        "                          max_p=2, max_d=2, max_q=2,\n",
        "                          max_P=2, max_D=1, max_Q=2,\n",
        "                          stepwise=True,\n",
        "                          suppress_warnings=True,\n",
        "                          error_action='ignore',\n",
        "                          information_criterion='aic',\n",
        "                          n_fits=50,\n",
        "                          start_p=0, start_q=0,\n",
        "                          start_P=0, start_Q=0)\n",
        "\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Error fitting final model: {e}\")\n",
        "        return None\n",
        "\n",
        "# Store results for comparison\n",
        "results = []\n",
        "all_selected_models = []\n",
        "\n",
        "print(\"Training SARIMA models using Hyndman-Khandakar Algorithm with 3-fold time series cross-validation...\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Data splitting: Use 1901-2010 for training/validation, 2011-2015 for final testing\n",
        "train_val_end = '2010-12-31'\n",
        "test_start = '2011-01-01'\n",
        "test_end = '2015-12-31'\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Lists to store CV scores across all subdivisions\n",
        "all_mae_scores = []\n",
        "all_rmse_scores = []\n",
        "successful_subdivisions = 0\n",
        "failed_subdivisions = 0\n",
        "subdivision_results = []\n",
        "\n",
        "print(f\"\\nProcessing {len(subdivisions)} subdivisions...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for i, subdivision in enumerate(subdivisions):\n",
        "    print(f\"Processing {subdivision} ({i+1}/{len(subdivisions)})...\", end=' ')\n",
        "\n",
        "    # Get data for the current subdivision\n",
        "    ts_data_subdivision = df_melted[df_melted['SUBDIVISION'] == subdivision]['Rainfall']\n",
        "    ts_data_subdivision = ts_data_subdivision.reindex(full_date_range)\n",
        "    ts_data_subdivision = ts_data_subdivision.ffill().bfill()\n",
        "\n",
        "    # Split data: training/validation period (1901-2010)\n",
        "    train_val_data = ts_data_subdivision[:'2010-12-31']\n",
        "\n",
        "    # Skip if insufficient data\n",
        "    if len(train_val_data) < 36:  # Need at least 3 years for 3-fold CV with seasonal data\n",
        "        failed_subdivisions += 1\n",
        "        print(\"FAILED - Insufficient data\")\n",
        "        continue\n",
        "\n",
        "    # Perform cross-validation using Hyndman-Khandakar algorithm\n",
        "    cv_mae, cv_rmse, selected_models = evaluate_hk_sarima_cv(train_val_data, n_splits=3)\n",
        "\n",
        "    if cv_mae is not None and cv_rmse is not None:\n",
        "        all_mae_scores.append(cv_mae)\n",
        "        all_rmse_scores.append(cv_rmse)\n",
        "        successful_subdivisions += 1\n",
        "\n",
        "        # Store detailed results for this subdivision\n",
        "        subdivision_results.append({\n",
        "            'subdivision': subdivision,\n",
        "            'cv_mae': cv_mae,\n",
        "            'cv_rmse': cv_rmse,\n",
        "            'selected_models': selected_models\n",
        "        })\n",
        "\n",
        "        all_selected_models.extend(selected_models)\n",
        "        print(f\"SUCCESS - CV MAE: {cv_mae:.2f}, CV RMSE: {cv_rmse:.2f}\")\n",
        "\n",
        "        # Print selected models for first few subdivisions\n",
        "        if i < 5 and selected_models:\n",
        "            most_common_order = max(set([str(m['order']) for m in selected_models]),\n",
        "                                  key=[str(m['order']) for m in selected_models].count)\n",
        "            most_common_seasonal = max(set([str(m['seasonal_order']) for m in selected_models]),\n",
        "                                     key=[str(m['seasonal_order']) for m in selected_models].count)\n",
        "            print(f\"    Most common order: {most_common_order}, seasonal: {most_common_seasonal}\")\n",
        "    else:\n",
        "        failed_subdivisions += 1\n",
        "        print(\"FAILED - CV error\")\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "\n",
        "# Calculate overall average CV scores\n",
        "if len(all_mae_scores) > 0:\n",
        "    avg_cv_mae = np.mean(all_mae_scores)\n",
        "    avg_cv_rmse = np.mean(all_rmse_scores)\n",
        "    std_cv_mae = np.std(all_mae_scores)\n",
        "    std_cv_rmse = np.std(all_rmse_scores)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(\"CROSS-VALIDATION RESULTS SUMMARY\")\n",
        "    print(\"=\" * 100)\n",
        "    print(f\"Average CV MAE: {avg_cv_mae:.3f} ± {std_cv_mae:.3f}\")\n",
        "    print(f\"Average CV RMSE: {avg_cv_rmse:.3f} ± {std_cv_rmse:.3f}\")\n",
        "    print(f\"Successful subdivisions: {successful_subdivisions}\")\n",
        "    print(f\"Failed subdivisions: {failed_subdivisions}\")\n",
        "    print(f\"Total processing time: {elapsed_time:.1f} seconds\")\n",
        "\n",
        "    # Analyze selected models\n",
        "    if all_selected_models:\n",
        "        print(f\"\\n\" + \"=\" * 60)\n",
        "        print(\"MODEL SELECTION ANALYSIS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        orders = [str(m['order']) for m in all_selected_models]\n",
        "        seasonal_orders = [str(m['seasonal_order']) for m in all_selected_models]\n",
        "\n",
        "        from collections import Counter\n",
        "        order_counts = Counter(orders)\n",
        "        seasonal_order_counts = Counter(seasonal_orders)\n",
        "\n",
        "        print(\"Most frequently selected ARIMA orders:\")\n",
        "        for order, count in order_counts.most_common(5):\n",
        "            percentage = (count / len(orders)) * 100\n",
        "            print(f\"  {order}: {count} times ({percentage:.1f}%)\")\n",
        "\n",
        "        print(\"\\nMost frequently selected seasonal orders:\")\n",
        "        for seasonal_order, count in seasonal_order_counts.most_common(5):\n",
        "            percentage = (count / len(seasonal_orders)) * 100\n",
        "            print(f\"  {seasonal_order}: {count} times ({percentage:.1f}%)\")\n",
        "\n",
        "    # Now evaluate on the held-out test set (2011-2015)\n",
        "    print(f\"\\n\" + \"=\" * 100)\n",
        "    print(\"FINAL EVALUATION ON TEST SET (2011-2015)\")\n",
        "    print(\"=\" * 100)\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    final_actuals = []\n",
        "    final_forecasts = []\n",
        "    final_successful = 0\n",
        "    final_failed = 0\n",
        "    test_results = []\n",
        "    final_models_used = []\n",
        "\n",
        "    print(f\"Fitting final models for {successful_subdivisions} successful subdivisions...\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    for i, subdivision in enumerate(subdivisions):\n",
        "        if subdivision not in [sr['subdivision'] for sr in subdivision_results]:\n",
        "            continue  # Skip subdivisions that failed in CV\n",
        "\n",
        "        print(f\"Final evaluation for {subdivision} ({final_successful + final_failed + 1}/{successful_subdivisions})...\", end=' ')\n",
        "\n",
        "        ts_data_subdivision = df_melted[df_melted['SUBDIVISION'] == subdivision]['Rainfall']\n",
        "        ts_data_subdivision = ts_data_subdivision.reindex(full_date_range)\n",
        "        ts_data_subdivision = ts_data_subdivision.ffill().bfill()\n",
        "\n",
        "        # Now use full training period (1901-2010) to predict test period (2011-2015)\n",
        "        train_data_final = ts_data_subdivision[:'2010-12-31']\n",
        "        test_data_final = ts_data_subdivision['2011-01-01':'2015-12-31']\n",
        "\n",
        "        if train_data_final.isnull().sum() == 0 and test_data_final.isnull().sum() == 0 and len(test_data_final) > 0:\n",
        "            try:\n",
        "                # Fit final model using Hyndman-Khandakar algorithm\n",
        "                final_model = fit_final_hk_sarima(train_data_final)\n",
        "\n",
        "                if final_model is not None:\n",
        "                    # Generate forecasts\n",
        "                    forecast_mean = final_model.predict(n_periods=len(test_data_final))\n",
        "                    forecast_mean.index = test_data_final.index\n",
        "\n",
        "                    final_actuals.extend(test_data_final.tolist())\n",
        "                    final_forecasts.extend(forecast_mean.tolist())\n",
        "                    final_successful += 1\n",
        "\n",
        "                    # Store model information\n",
        "                    final_models_used.append({\n",
        "                        'subdivision': subdivision,\n",
        "                        'order': final_model.order,\n",
        "                        'seasonal_order': final_model.seasonal_order,\n",
        "                        'aic': final_model.aic()\n",
        "                    })\n",
        "\n",
        "                    print(f\"SUCCESS - Model: SARIMA{final_model.order}x{final_model.seasonal_order}\")\n",
        "\n",
        "                    # Store sample results for display\n",
        "                    if len(test_results) < 30:  # Limit sample size\n",
        "                        for date, actual_value in test_data_final.items():\n",
        "                            forecast_value = forecast_mean.loc[date]\n",
        "                            year = date.year\n",
        "                            month_num = date.month\n",
        "                            month_name = expected_month_columns[month_num - 1]\n",
        "\n",
        "                            test_results.append({\n",
        "                                'SUBDIVISION': subdivision,\n",
        "                                'YEAR': year,\n",
        "                                'MONTH': month_name,\n",
        "                                'ACTUAL': actual_value,\n",
        "                                'FORECAST': forecast_value,\n",
        "                                'ERROR': abs(actual_value - forecast_value)\n",
        "                            })\n",
        "                else:\n",
        "                    final_failed += 1\n",
        "                    print(\"FAILED - Model fitting error\")\n",
        "\n",
        "            except Exception as e:\n",
        "                final_failed += 1\n",
        "                print(f\"FAILED - {str(e)[:50]}...\")\n",
        "                continue\n",
        "        else:\n",
        "            final_failed += 1\n",
        "            print(\"FAILED - Data quality issues\")\n",
        "\n",
        "    # Calculate final test metrics\n",
        "    if len(final_actuals) > 0 and len(final_forecasts) > 0:\n",
        "        final_mae = mean_absolute_error(final_actuals, final_forecasts)\n",
        "        final_rmse = np.sqrt(mean_squared_error(final_actuals, final_forecasts))\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 70)\n",
        "        print(\"FINAL TEST RESULTS\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Test MAE: {final_mae:.3f}\")\n",
        "        print(f\"Test RMSE: {final_rmse:.3f}\")\n",
        "        print(f\"Successful final predictions: {final_successful}\")\n",
        "        print(f\"Failed final predictions: {final_failed}\")\n",
        "        print(f\"Total test predictions: {len(final_actuals)}\")\n",
        "\n",
        "        # Show final model statistics\n",
        "        if final_models_used:\n",
        "            final_orders = [str(m['order']) for m in final_models_used]\n",
        "            final_seasonal_orders = [str(m['seasonal_order']) for m in final_models_used]\n",
        "\n",
        "            final_order_counts = Counter(final_orders)\n",
        "            final_seasonal_order_counts = Counter(final_seasonal_orders)\n",
        "\n",
        "            print(f\"\\nFinal models used (top 3):\")\n",
        "            for order, count in final_order_counts.most_common(3):\n",
        "                percentage = (count / len(final_orders)) * 100\n",
        "                print(f\"  ARIMA{order}: {count} subdivisions ({percentage:.1f}%)\")\n",
        "\n",
        "        # Display sample predictions\n",
        "        if test_results:\n",
        "            print(f\"\\nSample Test Predictions:\")\n",
        "            test_df = pd.DataFrame(test_results)\n",
        "            print(test_df.head(15).to_string(index=False, float_format='%.2f'))\n",
        "\n",
        "        print(f\"\\n\" + \"=\" * 70)\n",
        "        print(\"ALGORITHM COMPARISON\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Hyndman-Khandakar Algorithm Results:\")\n",
        "        print(f\"  - Processing time: {elapsed_time:.1f} seconds\")\n",
        "        print(f\"  - Models tested per subdivision: ~20-50 (adaptive)\")\n",
        "        print(f\"  - Final Test MAE: {final_mae:.3f}\")\n",
        "        print(f\"  - Final Test RMSE: {final_rmse:.3f}\")\n",
        "        print(f\"  - Success rate: {(final_successful/(final_successful+final_failed))*100:.1f}%\")\n",
        "        print(f\"\\nAdvantages over Grid Search:\")\n",
        "        print(f\"  - Statistically principled parameter selection\")\n",
        "        print(f\"  - Automatic handling of stationarity testing\")\n",
        "        print(f\"  - Much faster execution (~10-50x speedup)\")\n",
        "        print(f\"  - Built-in model validation\")\n",
        "\n",
        "    else:\n",
        "        print(\"No final test predictions could be made.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo valid subdivisions found during cross-validation.\")\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\nTotal execution time: {total_time:.1f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myebbU9a1O1H",
        "outputId": "e4548a94-625a-4490-b04d-ac5f2ab2c007"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SARIMA models using Hyndman-Khandakar Algorithm with 3-fold time series cross-validation...\n",
            "====================================================================================================\n",
            "\n",
            "Processing 33 subdivisions...\n",
            "------------------------------------------------------------\n",
            "Processing ASSAM & MEGHALAYA (1/33)... SUCCESS - CV MAE: 67.15, CV RMSE: 93.97\n",
            "    Most common order: (1, 0, 0), seasonal: (1, 0, 1, 12)\n",
            "Processing NORTH INTERIOR KARNATAKA (2/33)... SUCCESS - CV MAE: 26.25, CV RMSE: 38.49\n",
            "    Most common order: (1, 0, 0), seasonal: (1, 0, 2, 12)\n",
            "Processing UTTARAKHAND (3/33)... SUCCESS - CV MAE: 49.60, CV RMSE: 68.18\n",
            "    Most common order: (1, 0, 0), seasonal: (1, 0, 1, 12)\n",
            "Processing JAMMU & KASHMIR (4/33)... SUCCESS - CV MAE: 51.12, CV RMSE: 71.18\n",
            "    Most common order: (1, 0, 0), seasonal: (1, 0, 1, 12)\n",
            "Processing MADHYA MAHARASHTRA (5/33)... SUCCESS - CV MAE: 30.96, CV RMSE: 45.42\n",
            "    Most common order: (1, 0, 0), seasonal: (2, 0, 1, 12)\n",
            "Processing SUB HIMALAYAN WEST BENGAL & SIKKIM (6/33)... SUCCESS - CV MAE: 63.22, CV RMSE: 100.43\n",
            "Processing COASTAL KARNATAKA (7/33)... SUCCESS - CV MAE: 84.21, CV RMSE: 145.24\n",
            "Processing WEST UTTAR PRADESH (8/33)... SUCCESS - CV MAE: 54.12, CV RMSE: 76.20\n",
            "Processing WEST RAJASTHAN (9/33)... SUCCESS - CV MAE: 24.84, CV RMSE: 36.11\n",
            "Processing GUJARAT REGION (10/33)... SUCCESS - CV MAE: 56.78, CV RMSE: 88.68\n",
            "Processing TAMIL NADU (11/33)... SUCCESS - CV MAE: 32.32, CV RMSE: 44.07\n",
            "Processing MATATHWADA (12/33)... SUCCESS - CV MAE: 43.00, CV RMSE: 61.66\n",
            "Processing EAST RAJASTHAN (13/33)... SUCCESS - CV MAE: 36.32, CV RMSE: 52.03\n",
            "Processing BIHAR (14/33)... SUCCESS - CV MAE: 58.34, CV RMSE: 77.36\n",
            "Processing RAYALSEEMA (15/33)... SUCCESS - CV MAE: 31.43, CV RMSE: 45.41\n",
            "Processing TELANGANA (16/33)... SUCCESS - CV MAE: 36.39, CV RMSE: 59.01\n",
            "Processing GANGETIC WEST BENGAL (17/33)... SUCCESS - CV MAE: 44.79, CV RMSE: 67.17\n",
            "Processing HARYANA DELHI & CHANDIGARH (18/33)... SUCCESS - CV MAE: 36.26, CV RMSE: 56.79\n",
            "Processing WEST MADHYA PRADESH (19/33)... SUCCESS - CV MAE: 31.47, CV RMSE: 51.41\n",
            "Processing NAGA MANI MIZO TRIPURA (20/33)... SUCCESS - CV MAE: 69.20, CV RMSE: 102.17\n",
            "Processing COASTAL ANDHRA PRADESH (21/33)... SUCCESS - CV MAE: 41.33, CV RMSE: 63.09\n",
            "Processing EAST UTTAR PRADESH (22/33)... SUCCESS - CV MAE: 32.76, CV RMSE: 48.99\n",
            "Processing CHHATTISGARH (23/33)... SUCCESS - CV MAE: 86.57, CV RMSE: 110.27\n",
            "Processing EAST MADHYA PRADESH (24/33)... SUCCESS - CV MAE: 57.44, CV RMSE: 77.94\n",
            "Processing JHARKHAND (25/33)... SUCCESS - CV MAE: 62.63, CV RMSE: 81.73\n",
            "Processing PUNJAB (26/33)... SUCCESS - CV MAE: 28.34, CV RMSE: 48.45\n",
            "Processing VIDARBHA (27/33)... SUCCESS - CV MAE: 33.69, CV RMSE: 53.30\n",
            "Processing HIMACHAL PRADESH (28/33)... SUCCESS - CV MAE: 46.18, CV RMSE: 61.54\n",
            "Processing ORISSA (29/33)... SUCCESS - CV MAE: 37.34, CV RMSE: 54.87\n",
            "Processing KERALA (30/33)... SUCCESS - CV MAE: 98.27, CV RMSE: 144.07\n",
            "Processing KONKAN & GOA (31/33)... SUCCESS - CV MAE: 77.49, CV RMSE: 134.75\n",
            "Processing SAURASHTRA & KUTCH (32/33)... SUCCESS - CV MAE: 39.98, CV RMSE: 67.96\n",
            "Processing SOUTH INTERIOR KARNATAKA (33/33)... SUCCESS - CV MAE: 42.64, CV RMSE: 59.42\n",
            "\n",
            "====================================================================================================\n",
            "CROSS-VALIDATION RESULTS SUMMARY\n",
            "====================================================================================================\n",
            "Average CV MAE: 48.861 ± 18.563\n",
            "Average CV RMSE: 72.344 ± 28.431\n",
            "Successful subdivisions: 33\n",
            "Failed subdivisions: 0\n",
            "Total processing time: 7469.4 seconds\n",
            "\n",
            "============================================================\n",
            "MODEL SELECTION ANALYSIS\n",
            "============================================================\n",
            "Most frequently selected ARIMA orders:\n",
            "  (1, 0, 0): 35 times (53.0%)\n",
            "  (0, 0, 0): 12 times (18.2%)\n",
            "  (2, 0, 0): 8 times (12.1%)\n",
            "  (0, 0, 1): 5 times (7.6%)\n",
            "  (2, 0, 1): 2 times (3.0%)\n",
            "\n",
            "Most frequently selected seasonal orders:\n",
            "  (1, 0, 2, 12): 24 times (36.4%)\n",
            "  (1, 0, 1, 12): 20 times (30.3%)\n",
            "  (2, 0, 1, 12): 8 times (12.1%)\n",
            "  (2, 0, 0, 12): 8 times (12.1%)\n",
            "  (1, 0, 0, 12): 6 times (9.1%)\n",
            "\n",
            "====================================================================================================\n",
            "FINAL EVALUATION ON TEST SET (2011-2015)\n",
            "====================================================================================================\n",
            "Fitting final models for 33 successful subdivisions...\n",
            "----------------------------------------------------------------------\n",
            "Final evaluation for ASSAM & MEGHALAYA (1/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 1, 12)\n",
            "Final evaluation for NORTH INTERIOR KARNATAKA (2/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 2, 12)\n",
            "Final evaluation for UTTARAKHAND (3/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 2, 12)\n",
            "Final evaluation for JAMMU & KASHMIR (4/33)... SUCCESS - Model: SARIMA(2, 1, 0)x(2, 0, 0, 12)\n",
            "Final evaluation for MADHYA MAHARASHTRA (5/33)... SUCCESS - Model: SARIMA(1, 0, 1)x(2, 0, 1, 12)\n",
            "Final evaluation for SUB HIMALAYAN WEST BENGAL & SIKKIM (6/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(2, 0, 1, 12)\n",
            "Final evaluation for COASTAL KARNATAKA (7/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 1, 12)\n",
            "Final evaluation for WEST UTTAR PRADESH (8/33)... SUCCESS - Model: SARIMA(2, 0, 0)x(1, 0, 2, 12)\n",
            "Final evaluation for WEST RAJASTHAN (9/33)... SUCCESS - Model: SARIMA(2, 0, 1)x(2, 0, 0, 12)\n",
            "Final evaluation for GUJARAT REGION (10/33)... SUCCESS - Model: SARIMA(2, 0, 0)x(2, 0, 1, 12)\n",
            "Final evaluation for TAMIL NADU (11/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 1, 12)\n",
            "Final evaluation for MATATHWADA (12/33)... SUCCESS - Model: SARIMA(2, 0, 0)x(2, 0, 1, 12)\n",
            "Final evaluation for EAST RAJASTHAN (13/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(2, 0, 1, 12)\n",
            "Final evaluation for BIHAR (14/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 2, 12)\n",
            "Final evaluation for RAYALSEEMA (15/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 1, 12)\n",
            "Final evaluation for TELANGANA (16/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(2, 0, 1, 12)\n",
            "Final evaluation for GANGETIC WEST BENGAL (17/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 1, 12)\n",
            "Final evaluation for HARYANA DELHI & CHANDIGARH (18/33)... SUCCESS - Model: SARIMA(1, 0, 1)x(2, 0, 0, 12)\n",
            "Final evaluation for WEST MADHYA PRADESH (19/33)... SUCCESS - Model: SARIMA(0, 0, 1)x(2, 0, 1, 12)\n",
            "Final evaluation for NAGA MANI MIZO TRIPURA (20/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 2, 12)\n",
            "Final evaluation for COASTAL ANDHRA PRADESH (21/33)... SUCCESS - Model: SARIMA(2, 0, 0)x(1, 0, 1, 12)\n",
            "Final evaluation for EAST UTTAR PRADESH (22/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(2, 0, 1, 12)\n",
            "Final evaluation for CHHATTISGARH (23/33)... SUCCESS - Model: SARIMA(2, 0, 0)x(1, 0, 0, 12)\n",
            "Final evaluation for EAST MADHYA PRADESH (24/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 2, 12)\n",
            "Final evaluation for JHARKHAND (25/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 2, 12)\n",
            "Final evaluation for PUNJAB (26/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 2, 12)\n",
            "Final evaluation for VIDARBHA (27/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(2, 0, 1, 12)\n",
            "Final evaluation for HIMACHAL PRADESH (28/33)... SUCCESS - Model: SARIMA(0, 0, 1)x(1, 0, 1, 12)\n",
            "Final evaluation for ORISSA (29/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 0, 12)\n",
            "Final evaluation for KERALA (30/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 1, 12)\n",
            "Final evaluation for KONKAN & GOA (31/33)... SUCCESS - Model: SARIMA(0, 0, 2)x(1, 0, 2, 12)\n",
            "Final evaluation for SAURASHTRA & KUTCH (32/33)... SUCCESS - Model: SARIMA(1, 0, 1)x(2, 0, 0, 12)\n",
            "Final evaluation for SOUTH INTERIOR KARNATAKA (33/33)... SUCCESS - Model: SARIMA(1, 0, 0)x(1, 0, 1, 12)\n",
            "\n",
            "======================================================================\n",
            "FINAL TEST RESULTS\n",
            "======================================================================\n",
            "Test MAE: 41.578\n",
            "Test RMSE: 68.907\n",
            "Successful final predictions: 33\n",
            "Failed final predictions: 0\n",
            "Total test predictions: 1980\n",
            "\n",
            "Final models used (top 3):\n",
            "  ARIMA(1, 0, 0): 20 subdivisions (60.6%)\n",
            "  ARIMA(2, 0, 0): 5 subdivisions (15.2%)\n",
            "  ARIMA(1, 0, 1): 3 subdivisions (9.1%)\n",
            "\n",
            "Sample Test Predictions:\n",
            "      SUBDIVISION  YEAR MONTH  ACTUAL  FORECAST  ERROR\n",
            "ASSAM & MEGHALAYA  2011   JAN   11.10     16.05   4.95\n",
            "ASSAM & MEGHALAYA  2011   FEB   11.40     30.89  19.49\n",
            "ASSAM & MEGHALAYA  2011   MAR  109.00     79.50  29.50\n",
            "ASSAM & MEGHALAYA  2011   APR   92.10    218.47 126.37\n",
            "ASSAM & MEGHALAYA  2011   MAY  238.30    297.14  58.84\n",
            "ASSAM & MEGHALAYA  2011   JUN  316.00    455.07 139.07\n",
            "ASSAM & MEGHALAYA  2011   JUL  395.80    502.75 106.95\n",
            "ASSAM & MEGHALAYA  2011   AUG  302.60    394.85  92.25\n",
            "ASSAM & MEGHALAYA  2011   SEP  221.60    284.99  63.39\n",
            "ASSAM & MEGHALAYA  2011   OCT   30.20    158.07 127.87\n",
            "ASSAM & MEGHALAYA  2011   NOV   11.90     24.40  12.50\n",
            "ASSAM & MEGHALAYA  2011   DEC    3.50     11.00   7.50\n",
            "ASSAM & MEGHALAYA  2012   JAN   15.20     16.51   1.31\n",
            "ASSAM & MEGHALAYA  2012   FEB    6.90     31.30  24.40\n",
            "ASSAM & MEGHALAYA  2012   MAR   28.80     79.79  50.99\n",
            "\n",
            "======================================================================\n",
            "ALGORITHM COMPARISON\n",
            "======================================================================\n",
            "Hyndman-Khandakar Algorithm Results:\n",
            "  - Processing time: 7469.4 seconds\n",
            "  - Models tested per subdivision: ~20-50 (adaptive)\n",
            "  - Final Test MAE: 41.578\n",
            "  - Final Test RMSE: 68.907\n",
            "  - Success rate: 100.0%\n",
            "\n",
            "Advantages over Grid Search:\n",
            "  - Statistically principled parameter selection\n",
            "  - Automatic handling of stationarity testing\n",
            "  - Much faster execution (~10-50x speedup)\n",
            "  - Built-in model validation\n",
            "\n",
            "Total execution time: 13491.4 seconds\n"
          ]
        }
      ]
    }
  ]
}