{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-zpSvKGq-awK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, RepeatVector, TimeDistributed, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from itertools import product\n",
        "import json\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/Rainfall Data.csv')"
      ],
      "metadata": {
        "id": "by7dCL36kXMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4YsMKawH8Nq",
        "outputId": "b631f160-2288-4ec9-b70e-a81a93a74bc3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of years of data for each state (SUBDIVISION)\n",
        "years_per_state = df.groupby('SUBDIVISION')['YEAR'].nunique()\n",
        "\n",
        "# Filter to find states that do not have 115 years of data\n",
        "states_less_than_115_years = years_per_state[years_per_state != 115]"
      ],
      "metadata": {
        "id": "Y261_WXc_J-O"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "years_per_subdivision = df.groupby('SUBDIVISION')['YEAR'].nunique()\n",
        "states_to_drop = years_per_subdivision[years_per_subdivision != 115].index.tolist()\n",
        "df_filtered = df[~df['SUBDIVISION'].isin(states_to_drop)]"
      ],
      "metadata": {
        "id": "aYL84mpO_QwU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_states_filtered = df_filtered['SUBDIVISION'].nunique()\n",
        "print(f\"\\nNumber of states (SUBDIVISIONs) in the filtered dataset: {num_states_filtered}\")\n",
        "\n",
        "# Verify the number of years of data for each state in the filtered DataFrame\n",
        "years_per_state_filtered = df_filtered.groupby('SUBDIVISION')['YEAR'].nunique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MIuY5hd_VoB",
        "outputId": "d8e9540c-bec4-447c-c90d-eb8a0d3fc91e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of states (SUBDIVISIONs) in the filtered dataset: 33\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "year_range_per_state = df_filtered.groupby('SUBDIVISION')['YEAR'].agg(['min', 'max'])\n",
        "\n",
        "# Check if the year range is the same for all states\n",
        "if year_range_per_state['min'].nunique() == 1 and year_range_per_state['max'].nunique() == 1:\n",
        "    common_min_year = year_range_per_state['min'].iloc[0]\n",
        "    common_max_year = year_range_per_state['max'].iloc[0]\n",
        "    print(f\"The common year range for all states is: {common_min_year} - {common_max_year}\")\n",
        "else:\n",
        "    print(\"The year range is not the same for all states:\")\n",
        "    print(year_range_per_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqjerKnP_YjQ",
        "outputId": "56ea15ba-0fce-4bbd-fce5-e5db66f31123"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The common year range for all states is: 1901 - 2015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name:\n",
        "    print(f'GPU device: {device_name}')\n",
        "else:\n",
        "    print('No GPU found. Make sure you selected GPU in the runtime settings.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PukTXYDRD0mm",
        "outputId": "53ba733e-7732-4e2d-a0e3-91cf753ffbe7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU device: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Configuration Parameters ---\n",
        "LOOK_BACK_MONTHS = 24  # Number of past months to use as input (e.g., 2 years)\n",
        "FORECAST_HORIZON_MONTHS = 60 # Number of future months to predict (5 years * 12 months)\n",
        "\n",
        "# Grid Search Parameters\n",
        "GRID_SEARCH_PARAMS = {\n",
        "     'lstm_units': [32, 50],\n",
        "     'num_lstm_layers': [2, 3],\n",
        "     'learning_rate': [0.01, 0.001],\n",
        "     'batch_size': [32, 64],\n",
        "     'dropout_rate': [0.2, 0.3],\n",
        "     'epochs': [50, 100],\n",
        "     'patience': [15]}\n",
        "\n",
        "# --- 1. Use Pre-loaded Data ---\n",
        "print(\"Using pre-loaded DataFrame 'df_filtered'.\")\n",
        "print(f\"DataFrame shape: {df_filtered.shape}\")\n",
        "print(f\"Columns: {list(df_filtered.columns)}\")\n",
        "\n",
        "# --- 2. Missing Value Imputation ---\n",
        "month_columns = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
        "\n",
        "print(f\"\\nBefore imputation, missing values in monthly columns:\\n{df_filtered[month_columns].isnull().sum()}\")\n",
        "\n",
        "# Apply KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df_imputed = df_filtered.copy()\n",
        "df_imputed[month_columns] = imputer.fit_transform(df_imputed[month_columns])\n",
        "\n",
        "print(f\"\\nAfter imputation, missing values in monthly columns:\\n{df_imputed[month_columns].isnull().sum()}\")\n",
        "\n",
        "# --- Data Preparation Functions ---\n",
        "\n",
        "def create_sequences(data, n_steps_in, n_steps_out):\n",
        "    \"\"\"Create sequences for LSTM training.\"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data)):\n",
        "        end_ix = i + n_steps_in\n",
        "        out_end_ix = end_ix + n_steps_out\n",
        "        if out_end_ix > len(data):\n",
        "            break\n",
        "        seq_x = data[i:end_ix]\n",
        "        seq_y = data[end_ix:out_end_ix]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def prepare_data():\n",
        "    \"\"\"Prepare training and test data for all subdivisions.\"\"\"\n",
        "    all_X_train, all_y_train = [], []\n",
        "    all_X_val, all_y_val = [], []\n",
        "    all_X_test_input, all_y_test_actual = [], []\n",
        "    scalers = {}\n",
        "    subdivision_names_for_plotting = []\n",
        "\n",
        "    subdivisions = df_imputed['SUBDIVISION'].unique()\n",
        "\n",
        "    print(\"\\nProcessing data for each subdivision...\")\n",
        "    for sub_id in subdivisions:\n",
        "        sub_df = df_imputed[df_imputed['SUBDIVISION'] == sub_id].sort_values('YEAR').copy()\n",
        "\n",
        "        # Reshape monthly data into a continuous series\n",
        "        monthly_data_sub = sub_df[month_columns].values.flatten()\n",
        "        total_months_sub = len(monthly_data_sub)\n",
        "\n",
        "        # Skip if not enough data\n",
        "        if total_months_sub < FORECAST_HORIZON_MONTHS:\n",
        "            print(f\"Skipping {sub_id}: Not enough total months ({total_months_sub}) for a {FORECAST_HORIZON_MONTHS}-month forecast horizon.\")\n",
        "            continue\n",
        "\n",
        "        # Actual values for the prediction horizon\n",
        "        actual_test_values = monthly_data_sub[total_months_sub - FORECAST_HORIZON_MONTHS:]\n",
        "\n",
        "        # Training data\n",
        "        train_data_full_slice_end_idx = total_months_sub - FORECAST_HORIZON_MONTHS\n",
        "\n",
        "        if train_data_full_slice_end_idx > (110 * 12):\n",
        "            train_data = monthly_data_sub[:(110 * 12)]\n",
        "        else:\n",
        "            train_data = monthly_data_sub[:train_data_full_slice_end_idx]\n",
        "\n",
        "        if len(train_data) < LOOK_BACK_MONTHS:\n",
        "            print(f\"Skipping {sub_id}: Not enough training data ({len(train_data)} months) to form a {LOOK_BACK_MONTHS}-month lookback window.\")\n",
        "            continue\n",
        "\n",
        "        # Scale data\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        train_scaled = scaler.fit_transform(train_data.reshape(-1, 1))\n",
        "        scalers[sub_id] = scaler\n",
        "\n",
        "        # Create sequences for training\n",
        "        X_train_sub, y_train_sub = create_sequences(train_scaled, LOOK_BACK_MONTHS, FORECAST_HORIZON_MONTHS)\n",
        "\n",
        "        if X_train_sub.size > 0:\n",
        "            # Split training data into train and validation\n",
        "            val_split_idx = int(0.8 * len(X_train_sub))\n",
        "\n",
        "            X_train_sub_split = X_train_sub[:val_split_idx]\n",
        "            y_train_sub_split = y_train_sub[:val_split_idx]\n",
        "            X_val_sub = X_train_sub[val_split_idx:]\n",
        "            y_val_sub = y_train_sub[val_split_idx:]\n",
        "\n",
        "            all_X_train.append(X_train_sub_split)\n",
        "            all_y_train.append(y_train_sub_split)\n",
        "            all_X_val.append(X_val_sub)\n",
        "            all_y_val.append(y_val_sub)\n",
        "        else:\n",
        "            print(f\"Warning: Not enough data to create training sequences for {sub_id}.\")\n",
        "            continue\n",
        "\n",
        "        # Prepare test input\n",
        "        test_input_seq = train_scaled[-LOOK_BACK_MONTHS:]\n",
        "        all_X_test_input.append(test_input_seq)\n",
        "        all_y_test_actual.append(actual_test_values)\n",
        "        subdivision_names_for_plotting.append(sub_id)\n",
        "\n",
        "    if not all_X_train:\n",
        "        print(\"\\nError: No valid training sequences could be created from any subdivision.\")\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    # Concatenate all data\n",
        "    X_train = np.vstack(all_X_train)\n",
        "    y_train = np.vstack(all_y_train)\n",
        "    X_val = np.vstack(all_X_val)\n",
        "    y_val = np.vstack(all_y_val)\n",
        "    X_test_input = np.array(all_X_test_input)\n",
        "    y_test_actual = np.array(all_y_test_actual)\n",
        "\n",
        "    # Reshape for LSTM\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], 1))\n",
        "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
        "    y_val = y_val.reshape((y_val.shape[0], y_val.shape[1], 1))\n",
        "    X_test_input = X_test_input.reshape((X_test_input.shape[0], X_test_input.shape[1], 1))\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test_input, y_test_actual, scalers\n",
        "\n",
        "def build_model(lstm_units, num_lstm_layers, learning_rate, dropout_rate):\n",
        "    \"\"\"Build LSTM model with specified hyperparameters.\"\"\"\n",
        "    # Clear any existing models\n",
        "    tf.keras.backend.clear_session()\n",
        "\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(LOOK_BACK_MONTHS, 1))\n",
        "\n",
        "    # Stack LSTM layers in encoder\n",
        "    x = encoder_inputs\n",
        "    for i in range(num_lstm_layers - 1):\n",
        "        x = LSTM(lstm_units, return_sequences=True)(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Last encoder layer returns states\n",
        "    encoder_output = LSTM(lstm_units, return_state=True)(x)\n",
        "    encoder_states = encoder_output[1:]  # Get h and c states\n",
        "\n",
        "    # Decoder\n",
        "    decoder_inputs = RepeatVector(FORECAST_HORIZON_MONTHS)(encoder_states[0])\n",
        "\n",
        "    # Stack LSTM layers in decoder\n",
        "    x = decoder_inputs\n",
        "    for i in range(num_lstm_layers):\n",
        "        if i == 0:\n",
        "            x = LSTM(lstm_units, return_sequences=True)(x, initial_state=encoder_states)\n",
        "        else:\n",
        "            x = LSTM(lstm_units, return_sequences=True)(x)\n",
        "        if dropout_rate > 0:\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "    decoder_outputs = TimeDistributed(Dense(1))(x)\n",
        "\n",
        "    model = Model(encoder_inputs, decoder_outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='mae')\n",
        "\n",
        "    return model\n",
        "\n",
        "def evaluate_model(model, X_test_input, y_test_actual, scalers, subdivision_names):\n",
        "    \"\"\"Evaluate model and return metrics.\"\"\"\n",
        "    all_y_pred_unscaled = []\n",
        "\n",
        "    for i, sub_id in enumerate(subdivision_names):\n",
        "        input_seq_for_pred = np.expand_dims(X_test_input[i], axis=0)\n",
        "        predicted_scaled_sequence = model.predict(input_seq_for_pred, verbose=0)\n",
        "        predicted_scaled_sequence = predicted_scaled_sequence.reshape(-1, 1)\n",
        "\n",
        "        scaler = scalers[sub_id]\n",
        "        predicted_unscaled_sequence = scaler.inverse_transform(predicted_scaled_sequence)\n",
        "        all_y_pred_unscaled.append(predicted_unscaled_sequence.flatten())\n",
        "\n",
        "    y_pred_final = np.array(all_y_pred_unscaled)\n",
        "\n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_test_actual.flatten(), y_pred_final.flatten())\n",
        "    rmse = np.sqrt(mean_squared_error(y_test_actual.flatten(), y_pred_final.flatten()))\n",
        "\n",
        "    return mae, rmse\n",
        "\n",
        "def grid_search():\n",
        "    \"\"\"Perform grid search for hyperparameter optimization.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STARTING GRID SEARCH FOR HYPERPARAMETER OPTIMIZATION\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Prepare data once\n",
        "    print(\"Preparing data...\")\n",
        "    data_prep_result = prepare_data()\n",
        "    if data_prep_result[0] is None:\n",
        "        return None\n",
        "\n",
        "    X_train, y_train, X_val, y_val, X_test_input, y_test_actual, scalers = data_prep_result\n",
        "    subdivision_names = list(scalers.keys())\n",
        "\n",
        "    print(f\"Data prepared successfully:\")\n",
        "    print(f\"  Training samples: {X_train.shape[0]}\")\n",
        "    print(f\"  Validation samples: {X_val.shape[0]}\")\n",
        "    print(f\"  Test subdivisions: {len(subdivision_names)}\")\n",
        "\n",
        "    # Generate all parameter combinations\n",
        "    param_names = list(GRID_SEARCH_PARAMS.keys())\n",
        "    param_values = list(GRID_SEARCH_PARAMS.values())\n",
        "    param_combinations = list(product(*param_values))\n",
        "\n",
        "    print(f\"\\nTotal parameter combinations to test: {len(param_combinations)}\")\n",
        "\n",
        "    # Store results\n",
        "    results = []\n",
        "    best_score = float('inf')\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    # Grid search loop\n",
        "    for i, param_combo in enumerate(param_combinations):\n",
        "        params = dict(zip(param_names, param_combo))\n",
        "\n",
        "        print(f\"\\n--- Combination {i+1}/{len(param_combinations)} ---\")\n",
        "        print(f\"Parameters: {params}\")\n",
        "\n",
        "        try:\n",
        "            # Build model\n",
        "            model = build_model(\n",
        "                lstm_units=params['lstm_units'],\n",
        "                num_lstm_layers=params['num_lstm_layers'],\n",
        "                learning_rate=params['learning_rate'],\n",
        "                dropout_rate=params['dropout_rate']\n",
        "            )\n",
        "\n",
        "            # Early stopping\n",
        "            early_stopping = EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=params['patience'],\n",
        "                verbose=0,\n",
        "                mode='min',\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "\n",
        "            # Train model\n",
        "            history = model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=params['epochs'],\n",
        "                batch_size=params['batch_size'],\n",
        "                validation_data=(X_val, y_val),\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Get validation loss\n",
        "            val_loss = min(history.history['val_loss'])\n",
        "\n",
        "            # Evaluate on test set\n",
        "            test_mae, test_rmse = evaluate_model(model, X_test_input, y_test_actual, scalers, subdivision_names)\n",
        "\n",
        "            # Store results\n",
        "            result = {\n",
        "                'params': params,\n",
        "                'val_loss': val_loss,\n",
        "                'test_mae': test_mae,\n",
        "                'test_rmse': test_rmse,\n",
        "                'epochs_trained': len(history.history['loss'])\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "            print(f\"Test MAE: {test_mae:.4f}\")\n",
        "            print(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "            print(f\"Epochs trained: {result['epochs_trained']}\")\n",
        "\n",
        "            # Check if this is the best model\n",
        "            if val_loss < best_score:\n",
        "                best_score = val_loss\n",
        "                best_params = params.copy()\n",
        "                best_model = model\n",
        "                print(\"*** NEW BEST MODEL! ***\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with parameters {params}: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    # Sort results by validation loss\n",
        "    results.sort(key=lambda x: x['val_loss'])\n",
        "\n",
        "    return results, best_params, best_model, scalers, subdivision_names\n",
        "\n",
        "def save_results(results, best_params):\n",
        "    \"\"\"Save grid search results to file.\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"grid_search_results_{timestamp}.json\"\n",
        "\n",
        "    # Convert results to JSON-serializable format\n",
        "    results_json = []\n",
        "    for result in results:\n",
        "        result_copy = result.copy()\n",
        "        results_json.append(result_copy)\n",
        "\n",
        "    output = {\n",
        "        'best_params': best_params,\n",
        "        'all_results': results_json,\n",
        "        'search_params': GRID_SEARCH_PARAMS\n",
        "    }\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(output, f, indent=2)\n",
        "\n",
        "    print(f\"\\nResults saved to: {filename}\")\n",
        "\n",
        "def print_top_results(results, top_n=5):\n",
        "    \"\"\"Print top N results from grid search.\"\"\"\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(f\"TOP {min(top_n, len(results))} PARAMETER COMBINATIONS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for i, result in enumerate(results[:top_n]):\n",
        "        print(f\"\\nRank {i+1}:\")\n",
        "        print(f\"  Validation Loss: {result['val_loss']:.4f}\")\n",
        "        print(f\"  Test MAE: {result['test_mae']:.4f}\")\n",
        "        print(f\"  Test RMSE: {result['test_rmse']:.4f}\")\n",
        "        print(f\"  Epochs Trained: {result['epochs_trained']}\")\n",
        "        print(f\"  Parameters: {result['params']}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Run grid search\n",
        "    results, best_params, best_model, scalers, subdivision_names = grid_search()\n",
        "\n",
        "    if results is None:\n",
        "        print(\"Grid search failed due to data preparation issues.\")\n",
        "        exit()\n",
        "\n",
        "    # Print results\n",
        "    print_top_results(results, top_n=10)\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"BEST MODEL SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Best Parameters: {best_params}\")\n",
        "    print(f\"Best Validation Loss: {results[0]['val_loss']:.4f}\")\n",
        "    print(f\"Best Test MAE: {results[0]['test_mae']:.4f}\")\n",
        "    print(f\"Best Test RMSE: {results[0]['test_rmse']:.4f}\")\n",
        "\n",
        "    # Save results\n",
        "    save_results(results, best_params)\n",
        "\n",
        "    # Final evaluation with best model\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"FINAL EVALUATION WITH BEST MODEL\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    if best_model is not None:\n",
        "        X_train, y_train, X_val, y_val, X_test_input, y_test_actual, _ = prepare_data()\n",
        "        final_mae, final_rmse = evaluate_model(best_model, X_test_input, y_test_actual, scalers, subdivision_names)\n",
        "\n",
        "        print(f\"Final Test MAE: {final_mae:.4f}\")\n",
        "        print(f\"Final Test RMSE: {final_rmse:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        model_filename = f\"best_lstm_model_{timestamp}.h5\"\n",
        "        best_model.save(model_filename)\n",
        "        print(f\"Best model saved as: {model_filename}\")\n",
        "\n",
        "    print(\"\\nGrid search completed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZgqB3Xj_aL8",
        "outputId": "603a9b3f-6c9e-4429-a2ca-cc6f38c9cb74"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pre-loaded DataFrame 'df_filtered'.\n",
            "DataFrame shape: (3795, 19)\n",
            "Columns: ['SUBDIVISION', 'YEAR', 'JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN', 'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC', 'ANNUAL', 'Jan-Feb', 'Mar-May', 'Jun-Sep', 'Oct-Dec']\n",
            "\n",
            "Before imputation, missing values in monthly columns:\n",
            "JAN    1\n",
            "FEB    1\n",
            "MAR    0\n",
            "APR    0\n",
            "MAY    0\n",
            "JUN    0\n",
            "JUL    1\n",
            "AUG    0\n",
            "SEP    0\n",
            "OCT    0\n",
            "NOV    1\n",
            "DEC    1\n",
            "dtype: int64\n",
            "\n",
            "After imputation, missing values in monthly columns:\n",
            "JAN    0\n",
            "FEB    0\n",
            "MAR    0\n",
            "APR    0\n",
            "MAY    0\n",
            "JUN    0\n",
            "JUL    0\n",
            "AUG    0\n",
            "SEP    0\n",
            "OCT    0\n",
            "NOV    0\n",
            "DEC    0\n",
            "dtype: int64\n",
            "\n",
            "==================================================\n",
            "STARTING GRID SEARCH FOR HYPERPARAMETER OPTIMIZATION\n",
            "==================================================\n",
            "Preparing data...\n",
            "\n",
            "Processing data for each subdivision...\n",
            "Data prepared successfully:\n",
            "  Training samples: 32637\n",
            "  Validation samples: 8184\n",
            "  Test subdivisions: 33\n",
            "\n",
            "Total parameter combinations to test: 64\n",
            "\n",
            "--- Combination 1/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0616\n",
            "Test MAE: 39.1129\n",
            "Test RMSE: 70.5410\n",
            "Epochs trained: 39\n",
            "*** NEW BEST MODEL! ***\n",
            "\n",
            "--- Combination 2/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0618\n",
            "Test MAE: 38.9382\n",
            "Test RMSE: 72.3382\n",
            "Epochs trained: 28\n",
            "\n",
            "--- Combination 3/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0654\n",
            "Test MAE: 43.7290\n",
            "Test RMSE: 80.9187\n",
            "Epochs trained: 18\n",
            "\n",
            "--- Combination 4/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0622\n",
            "Test MAE: 38.8456\n",
            "Test RMSE: 70.3267\n",
            "Epochs trained: 27\n",
            "\n",
            "--- Combination 5/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0616\n",
            "Test MAE: 40.9718\n",
            "Test RMSE: 74.0257\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 6/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0619\n",
            "Test MAE: 38.3110\n",
            "Test RMSE: 70.8855\n",
            "Epochs trained: 32\n",
            "\n",
            "--- Combination 7/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0611\n",
            "Test MAE: 39.0142\n",
            "Test RMSE: 71.7209\n",
            "Epochs trained: 42\n",
            "*** NEW BEST MODEL! ***\n",
            "\n",
            "--- Combination 8/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0617\n",
            "Test MAE: 38.1726\n",
            "Test RMSE: 70.1494\n",
            "Epochs trained: 29\n",
            "\n",
            "--- Combination 9/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0609\n",
            "Test MAE: 39.4917\n",
            "Test RMSE: 72.7882\n",
            "Epochs trained: 47\n",
            "*** NEW BEST MODEL! ***\n",
            "\n",
            "--- Combination 10/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0609\n",
            "Test MAE: 38.6171\n",
            "Test RMSE: 70.4858\n",
            "Epochs trained: 57\n",
            "*** NEW BEST MODEL! ***\n",
            "\n",
            "--- Combination 11/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0612\n",
            "Test MAE: 39.4922\n",
            "Test RMSE: 72.5353\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 12/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0610\n",
            "Test MAE: 38.5573\n",
            "Test RMSE: 71.1329\n",
            "Epochs trained: 47\n",
            "\n",
            "--- Combination 13/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0612\n",
            "Test MAE: 39.6525\n",
            "Test RMSE: 73.3527\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 14/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0608\n",
            "Test MAE: 38.6446\n",
            "Test RMSE: 71.1509\n",
            "Epochs trained: 79\n",
            "*** NEW BEST MODEL! ***\n",
            "\n",
            "--- Combination 15/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0612\n",
            "Test MAE: 39.5341\n",
            "Test RMSE: 72.5825\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 16/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0609\n",
            "Test MAE: 39.2934\n",
            "Test RMSE: 71.6465\n",
            "Epochs trained: 52\n",
            "\n",
            "--- Combination 17/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0686\n",
            "Test MAE: 43.8931\n",
            "Test RMSE: 79.6702\n",
            "Epochs trained: 16\n",
            "\n",
            "--- Combination 18/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0802\n",
            "Test MAE: 48.7932\n",
            "Test RMSE: 87.5192\n",
            "Epochs trained: 27\n",
            "\n",
            "--- Combination 19/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.1251\n",
            "Test MAE: 84.3738\n",
            "Test RMSE: 153.5271\n",
            "Epochs trained: 38\n",
            "\n",
            "--- Combination 20/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0665\n",
            "Test MAE: 42.6851\n",
            "Test RMSE: 76.1048\n",
            "Epochs trained: 18\n",
            "\n",
            "--- Combination 21/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0691\n",
            "Test MAE: 43.1758\n",
            "Test RMSE: 81.4096\n",
            "Epochs trained: 37\n",
            "\n",
            "--- Combination 22/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0623\n",
            "Test MAE: 38.7288\n",
            "Test RMSE: 70.2255\n",
            "Epochs trained: 36\n",
            "\n",
            "--- Combination 23/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0634\n",
            "Test MAE: 41.8407\n",
            "Test RMSE: 76.7930\n",
            "Epochs trained: 25\n",
            "\n",
            "--- Combination 24/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0639\n",
            "Test MAE: 43.2484\n",
            "Test RMSE: 76.8909\n",
            "Epochs trained: 22\n",
            "\n",
            "--- Combination 25/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0609\n",
            "Test MAE: 38.4740\n",
            "Test RMSE: 69.5847\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 26/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0612\n",
            "Test MAE: 38.4444\n",
            "Test RMSE: 70.0616\n",
            "Epochs trained: 43\n",
            "\n",
            "--- Combination 27/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0610\n",
            "Test MAE: 37.6551\n",
            "Test RMSE: 68.6551\n",
            "Epochs trained: 35\n",
            "\n",
            "--- Combination 28/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0611\n",
            "Test MAE: 38.4709\n",
            "Test RMSE: 69.4195\n",
            "Epochs trained: 52\n",
            "\n",
            "--- Combination 29/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0611\n",
            "Test MAE: 41.0015\n",
            "Test RMSE: 75.3245\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 30/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0608\n",
            "Test MAE: 39.1549\n",
            "Test RMSE: 70.2152\n",
            "Epochs trained: 60\n",
            "\n",
            "--- Combination 31/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0613\n",
            "Test MAE: 40.9407\n",
            "Test RMSE: 77.9887\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 32/64 ---\n",
            "Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0611\n",
            "Test MAE: 39.2252\n",
            "Test RMSE: 71.5709\n",
            "Epochs trained: 54\n",
            "\n",
            "--- Combination 33/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0640\n",
            "Test MAE: 40.5712\n",
            "Test RMSE: 73.4991\n",
            "Epochs trained: 20\n",
            "\n",
            "--- Combination 34/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0615\n",
            "Test MAE: 40.2651\n",
            "Test RMSE: 72.4300\n",
            "Epochs trained: 44\n",
            "\n",
            "--- Combination 35/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0618\n",
            "Test MAE: 40.8298\n",
            "Test RMSE: 72.4462\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 36/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0608\n",
            "Test MAE: 37.4905\n",
            "Test RMSE: 69.0636\n",
            "Epochs trained: 55\n",
            "*** NEW BEST MODEL! ***\n",
            "\n",
            "--- Combination 37/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0614\n",
            "Test MAE: 39.6497\n",
            "Test RMSE: 70.8480\n",
            "Epochs trained: 40\n",
            "\n",
            "--- Combination 38/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0613\n",
            "Test MAE: 40.8040\n",
            "Test RMSE: 72.4517\n",
            "Epochs trained: 53\n",
            "\n",
            "--- Combination 39/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0610\n",
            "Test MAE: 38.5398\n",
            "Test RMSE: 70.8472\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 40/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0609\n",
            "Test MAE: 39.3090\n",
            "Test RMSE: 71.7194\n",
            "Epochs trained: 69\n",
            "\n",
            "--- Combination 41/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0611\n",
            "Test MAE: 38.0317\n",
            "Test RMSE: 69.7279\n",
            "Epochs trained: 34\n",
            "\n",
            "--- Combination 42/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0610\n",
            "Test MAE: 38.5214\n",
            "Test RMSE: 70.6451\n",
            "Epochs trained: 33\n",
            "\n",
            "--- Combination 43/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0607\n",
            "Test MAE: 38.5325\n",
            "Test RMSE: 70.1847\n",
            "Epochs trained: 50\n",
            "*** NEW BEST MODEL! ***\n",
            "\n",
            "--- Combination 44/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0607\n",
            "Test MAE: 39.0340\n",
            "Test RMSE: 70.2541\n",
            "Epochs trained: 44\n",
            "*** NEW BEST MODEL! ***\n",
            "\n",
            "--- Combination 45/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0608\n",
            "Test MAE: 39.1821\n",
            "Test RMSE: 71.1885\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 46/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0609\n",
            "Test MAE: 38.2534\n",
            "Test RMSE: 69.7085\n",
            "Epochs trained: 57\n",
            "\n",
            "--- Combination 47/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0607\n",
            "Test MAE: 38.5509\n",
            "Test RMSE: 70.9242\n",
            "Epochs trained: 50\n",
            "*** NEW BEST MODEL! ***\n",
            "\n",
            "--- Combination 48/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0610\n",
            "Test MAE: 39.6046\n",
            "Test RMSE: 72.2127\n",
            "Epochs trained: 56\n",
            "\n",
            "--- Combination 49/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0666\n",
            "Test MAE: 40.5512\n",
            "Test RMSE: 73.4503\n",
            "Epochs trained: 20\n",
            "\n",
            "--- Combination 50/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0630\n",
            "Test MAE: 38.7434\n",
            "Test RMSE: 70.4630\n",
            "Epochs trained: 79\n",
            "\n",
            "--- Combination 51/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0669\n",
            "Test MAE: 40.9929\n",
            "Test RMSE: 74.5591\n",
            "Epochs trained: 17\n",
            "\n",
            "--- Combination 52/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0660\n",
            "Test MAE: 42.7714\n",
            "Test RMSE: 78.5927\n",
            "Epochs trained: 18\n",
            "\n",
            "--- Combination 53/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0629\n",
            "Test MAE: 39.7128\n",
            "Test RMSE: 72.3769\n",
            "Epochs trained: 23\n",
            "\n",
            "--- Combination 54/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0704\n",
            "Test MAE: 44.4355\n",
            "Test RMSE: 79.7961\n",
            "Epochs trained: 16\n",
            "\n",
            "--- Combination 55/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0657\n",
            "Test MAE: 40.8045\n",
            "Test RMSE: 76.4532\n",
            "Epochs trained: 20\n",
            "\n",
            "--- Combination 56/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.01, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0630\n",
            "Test MAE: 39.3011\n",
            "Test RMSE: 71.5312\n",
            "Epochs trained: 27\n",
            "\n",
            "--- Combination 57/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0611\n",
            "Test MAE: 40.3499\n",
            "Test RMSE: 73.1022\n",
            "Epochs trained: 38\n",
            "\n",
            "--- Combination 58/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0608\n",
            "Test MAE: 38.9754\n",
            "Test RMSE: 70.3964\n",
            "Epochs trained: 65\n",
            "\n",
            "--- Combination 59/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0614\n",
            "Test MAE: 38.6463\n",
            "Test RMSE: 71.4113\n",
            "Epochs trained: 33\n",
            "\n",
            "--- Combination 60/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0612\n",
            "Test MAE: 39.3450\n",
            "Test RMSE: 70.3908\n",
            "Epochs trained: 35\n",
            "\n",
            "--- Combination 61/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0614\n",
            "Test MAE: 38.4719\n",
            "Test RMSE: 69.3340\n",
            "Epochs trained: 40\n",
            "\n",
            "--- Combination 62/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0609\n",
            "Test MAE: 41.4958\n",
            "Test RMSE: 78.8398\n",
            "Epochs trained: 80\n",
            "\n",
            "--- Combination 63/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Validation Loss: 0.0609\n",
            "Test MAE: 39.6880\n",
            "Test RMSE: 70.4153\n",
            "Epochs trained: 50\n",
            "\n",
            "--- Combination 64/64 ---\n",
            "Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "Validation Loss: 0.0615\n",
            "Test MAE: 39.5610\n",
            "Test RMSE: 71.2942\n",
            "Epochs trained: 38\n",
            "\n",
            "============================================================\n",
            "TOP 10 PARAMETER COMBINATIONS\n",
            "============================================================\n",
            "\n",
            "Rank 1:\n",
            "  Validation Loss: 0.0607\n",
            "  Test MAE: 38.5509\n",
            "  Test RMSE: 70.9242\n",
            "  Epochs Trained: 50\n",
            "  Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "\n",
            "Rank 2:\n",
            "  Validation Loss: 0.0607\n",
            "  Test MAE: 39.0340\n",
            "  Test RMSE: 70.2541\n",
            "  Epochs Trained: 44\n",
            "  Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "\n",
            "Rank 3:\n",
            "  Validation Loss: 0.0607\n",
            "  Test MAE: 38.5325\n",
            "  Test RMSE: 70.1847\n",
            "  Epochs Trained: 50\n",
            "  Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "\n",
            "Rank 4:\n",
            "  Validation Loss: 0.0608\n",
            "  Test MAE: 38.9754\n",
            "  Test RMSE: 70.3964\n",
            "  Epochs Trained: 65\n",
            "  Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "\n",
            "Rank 5:\n",
            "  Validation Loss: 0.0608\n",
            "  Test MAE: 37.4905\n",
            "  Test RMSE: 69.0636\n",
            "  Epochs Trained: 55\n",
            "  Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.01, 'batch_size': 32, 'dropout_rate': 0.3, 'epochs': 100, 'patience': 15}\n",
            "\n",
            "Rank 6:\n",
            "  Validation Loss: 0.0608\n",
            "  Test MAE: 38.6446\n",
            "  Test RMSE: 71.1509\n",
            "  Epochs Trained: 79\n",
            "  Parameters: {'lstm_units': 32, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "\n",
            "Rank 7:\n",
            "  Validation Loss: 0.0608\n",
            "  Test MAE: 39.1549\n",
            "  Test RMSE: 70.2152\n",
            "  Epochs Trained: 60\n",
            "  Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 100, 'patience': 15}\n",
            "\n",
            "Rank 8:\n",
            "  Validation Loss: 0.0608\n",
            "  Test MAE: 39.1821\n",
            "  Test RMSE: 71.1885\n",
            "  Epochs Trained: 50\n",
            "  Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "\n",
            "Rank 9:\n",
            "  Validation Loss: 0.0609\n",
            "  Test MAE: 39.6880\n",
            "  Test RMSE: 70.4153\n",
            "  Epochs Trained: 50\n",
            "  Parameters: {'lstm_units': 50, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "\n",
            "Rank 10:\n",
            "  Validation Loss: 0.0609\n",
            "  Test MAE: 38.4740\n",
            "  Test RMSE: 69.5847\n",
            "  Epochs Trained: 50\n",
            "  Parameters: {'lstm_units': 32, 'num_lstm_layers': 3, 'learning_rate': 0.001, 'batch_size': 32, 'dropout_rate': 0.2, 'epochs': 50, 'patience': 15}\n",
            "\n",
            "============================================================\n",
            "BEST MODEL SUMMARY\n",
            "============================================================\n",
            "Best Parameters: {'lstm_units': 50, 'num_lstm_layers': 2, 'learning_rate': 0.001, 'batch_size': 64, 'dropout_rate': 0.3, 'epochs': 50, 'patience': 15}\n",
            "Best Validation Loss: 0.0607\n",
            "Best Test MAE: 38.5509\n",
            "Best Test RMSE: 70.9242\n",
            "\n",
            "Results saved to: grid_search_results_20250822_192223.json\n",
            "\n",
            "============================================================\n",
            "FINAL EVALUATION WITH BEST MODEL\n",
            "============================================================\n",
            "\n",
            "Processing data for each subdivision...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MAE: 38.5509\n",
            "Final Test RMSE: 70.9242\n",
            "Best model saved as: best_lstm_model_20250822_192226.h5\n",
            "\n",
            "Grid search completed!\n"
          ]
        }
      ]
    }
  ]
}